{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q_Learning\n",
    "#### Terminologies in Q-Learning\n",
    "- *States(S)* : the current position of the agent in the environment.\n",
    "- *Action(A)* : a step taken by the agent in a particular state.\n",
    "- *Rewards* : for every action, the agent receives a reward and penalty from environment.\n",
    "- *Episodes*  : the end of the stage, where agent can't take new action. It happends when the agent has achieved the goal or failed.\n",
    "- *Q(S_{ t+1 }, At)* : expected optimal Q-value of doing the action in a particular state.\n",
    "- *Q(St, At)* : it is the current estimation of Q(St+1,A).\n",
    "- *Q-Table* : the agent maintains the Q-Table of sets of states and actions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f788b1f3490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import imageio\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training episodes\n",
    "tre = 10000\n",
    "# Epsilon\n",
    "max_e = 1.0\n",
    "min_e = .05\n",
    "dr = 0.0005\n",
    "# max steps\n",
    "ms = 100\n",
    "# Gamma\n",
    "g = .95\n",
    "# Learning rate\n",
    "lr = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tre, min_e, max_e, dr, ms, env, Q, lr, g):\n",
    "    for i in trange(tre):\n",
    "        \"\"\"\n",
    "        this function train the Frozen Lake model with Q_Learning algorithms\n",
    "        \"\"\"\n",
    "        e = min_e + (max_e -min_e) * np.exp(-dr * i)\n",
    "        s = env.reset()[0]\n",
    "        done = False\n",
    "        for _  in range(ms):\n",
    "            # action Greedy\n",
    "            rnd = random.uniform(0,1)\n",
    "            if rnd > e:\n",
    "                a = np.argmax(Q[s])\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "            # new State\n",
    "            s_, r, done, _ , _2= env.step(a)\n",
    "            # Bellman Equition\n",
    "            Q[s,a] = Q[s,a] + lr * (r + g * np.max(Q[s_, :]) - Q[s,a])\n",
    "            # If DONE == TRUE -> break the loop\n",
    "            if done:\n",
    "                break\n",
    "            # Add new State to State in inner for loop\n",
    "            s = s_\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b034353a344188be89a65f3d7f00f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reza/rl/venv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "Q =train(tre, min_e, max_e, dr, ms, env, Q, lr, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.7737808 , 0.77378089],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
